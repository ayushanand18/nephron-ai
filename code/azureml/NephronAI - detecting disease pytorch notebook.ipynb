{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Nephron AI\n## Fighting chronic kidney disease using deep learning and fuzzy logic\n\nChronic kidney disease, or CKD, causes more deaths than breast cancer or prostate cancer. It is the under-recognized public health crisis. It affects an estimated 37 million people in the U.S. (15% of the adult population; more than 1 in 7 adults) and approximately 90% of those with CKD don’t even know they have it. 1 in 3 American adults (approximately 80 million people) is at risk for CKD. CKD is more common in women (15%) than men (12%). CKD is the 9th leading cause of death in the U.S. In 2016, over 500,000 patients received dialysis treatment, and over 200,000 lived with a kidney transplant.\n\n**NephronAI** works in three steps\n- Predict the disease\n- Predict the cause\n- Recovery strategy\n\n**For the prediction of the disease**, we are using a deep learning neural network built on PyTorch that yields more than 99.6% acduracy. Actual model is deployed on Azure Machine Learning.\n\n### Why are we using PyTorch?\nThe primary reason is that PyTorch uses dynamic computation graphs while Tensorflow (or Keras, or Theano) uses static graphs.Its favoured in research and we are using it for the same purpose."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step -1 Importing data\n\nWe are using a data set provided by Apollo Hospitals Chennai available publicly on the internet. For more information or resource links refer to the GitHub repository [https://github.com/ayushanand18/nephron-ai](https://github.com/ayushanand18/nephron-ai)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv('Chronic Kidney Disease Data - Feature Selected & Cleaned MICE (Original).csv', header='infer')\ndf=pd.DataFrame(data)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "df.head(10)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>hemo</th>\n      <th>sc</th>\n      <th>pcv</th>\n      <th>al</th>\n      <th>rc</th>\n      <th>htn</th>\n      <th>sg</th>\n      <th>dm</th>\n      <th>sod</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ckd</td>\n      <td>15.4</td>\n      <td>1.2</td>\n      <td>44.0</td>\n      <td>1.0</td>\n      <td>5.200000</td>\n      <td>yes</td>\n      <td>1.02</td>\n      <td>yes</td>\n      <td>146.657867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ckd</td>\n      <td>11.3</td>\n      <td>0.8</td>\n      <td>38.0</td>\n      <td>4.0</td>\n      <td>4.686921</td>\n      <td>no</td>\n      <td>1.02</td>\n      <td>no</td>\n      <td>151.385254</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ckd</td>\n      <td>9.6</td>\n      <td>1.8</td>\n      <td>31.0</td>\n      <td>2.0</td>\n      <td>4.114644</td>\n      <td>no</td>\n      <td>1.01</td>\n      <td>yes</td>\n      <td>130.725586</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ckd</td>\n      <td>11.2</td>\n      <td>3.8</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>3.900000</td>\n      <td>yes</td>\n      <td>1.01</td>\n      <td>no</td>\n      <td>111.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ckd</td>\n      <td>11.6</td>\n      <td>1.4</td>\n      <td>35.0</td>\n      <td>2.0</td>\n      <td>4.600000</td>\n      <td>no</td>\n      <td>1.01</td>\n      <td>no</td>\n      <td>137.998795</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ckd</td>\n      <td>12.2</td>\n      <td>1.1</td>\n      <td>39.0</td>\n      <td>3.0</td>\n      <td>4.400000</td>\n      <td>yes</td>\n      <td>1.02</td>\n      <td>yes</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ckd</td>\n      <td>12.4</td>\n      <td>24.0</td>\n      <td>36.0</td>\n      <td>0.0</td>\n      <td>4.188086</td>\n      <td>no</td>\n      <td>1.01</td>\n      <td>no</td>\n      <td>104.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ckd</td>\n      <td>12.4</td>\n      <td>1.1</td>\n      <td>44.0</td>\n      <td>2.0</td>\n      <td>5.000000</td>\n      <td>no</td>\n      <td>1.02</td>\n      <td>yes</td>\n      <td>146.358780</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ckd</td>\n      <td>10.8</td>\n      <td>1.9</td>\n      <td>33.0</td>\n      <td>3.0</td>\n      <td>4.000000</td>\n      <td>yes</td>\n      <td>1.02</td>\n      <td>yes</td>\n      <td>140.041412</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ckd</td>\n      <td>9.5</td>\n      <td>7.2</td>\n      <td>29.0</td>\n      <td>2.0</td>\n      <td>3.700000</td>\n      <td>yes</td>\n      <td>1.02</td>\n      <td>yes</td>\n      <td>114.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "  classification  hemo    sc   pcv   al        rc  htn    sg   dm         sod\n0            ckd  15.4   1.2  44.0  1.0  5.200000  yes  1.02  yes  146.657867\n1            ckd  11.3   0.8  38.0  4.0  4.686921   no  1.02   no  151.385254\n2            ckd   9.6   1.8  31.0  2.0  4.114644   no  1.01  yes  130.725586\n3            ckd  11.2   3.8  32.0  4.0  3.900000  yes  1.01   no  111.000000\n4            ckd  11.6   1.4  35.0  2.0  4.600000   no  1.01   no  137.998795\n5            ckd  12.2   1.1  39.0  3.0  4.400000  yes  1.02  yes  142.000000\n6            ckd  12.4  24.0  36.0  0.0  4.188086   no  1.01   no  104.000000\n7            ckd  12.4   1.1  44.0  2.0  5.000000   no  1.02  yes  146.358780\n8            ckd  10.8   1.9  33.0  3.0  4.000000  yes  1.02  yes  140.041412\n9            ckd   9.5   7.2  29.0  2.0  3.700000  yes  1.02  yes  114.000000"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's look at the data. We are using seaborn library for visualizing using *pairplot*."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import seaborn as sb\nsb.pairplot(df,hue=\"classification\")",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "<seaborn.axisgrid.PairGrid at 0x7f2a08178048>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "df.replace('yes',True)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>hemo</th>\n      <th>sc</th>\n      <th>pcv</th>\n      <th>al</th>\n      <th>rc</th>\n      <th>htn</th>\n      <th>sg</th>\n      <th>dm</th>\n      <th>sod</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ckd</td>\n      <td>15.400000</td>\n      <td>1.200000</td>\n      <td>44.000000</td>\n      <td>1.000000</td>\n      <td>5.200000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>146.657867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ckd</td>\n      <td>11.300000</td>\n      <td>0.800000</td>\n      <td>38.000000</td>\n      <td>4.000000</td>\n      <td>4.686921</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>151.385254</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ckd</td>\n      <td>9.600000</td>\n      <td>1.800000</td>\n      <td>31.000000</td>\n      <td>2.000000</td>\n      <td>4.114644</td>\n      <td>no</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.725586</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ckd</td>\n      <td>11.200000</td>\n      <td>3.800000</td>\n      <td>32.000000</td>\n      <td>4.000000</td>\n      <td>3.900000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>no</td>\n      <td>111.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ckd</td>\n      <td>11.600000</td>\n      <td>1.400000</td>\n      <td>35.000000</td>\n      <td>2.000000</td>\n      <td>4.600000</td>\n      <td>no</td>\n      <td>1.010000</td>\n      <td>no</td>\n      <td>137.998795</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ckd</td>\n      <td>12.200000</td>\n      <td>1.100000</td>\n      <td>39.000000</td>\n      <td>3.000000</td>\n      <td>4.400000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ckd</td>\n      <td>12.400000</td>\n      <td>24.000000</td>\n      <td>36.000000</td>\n      <td>0.000000</td>\n      <td>4.188086</td>\n      <td>no</td>\n      <td>1.010000</td>\n      <td>no</td>\n      <td>104.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ckd</td>\n      <td>12.400000</td>\n      <td>1.100000</td>\n      <td>44.000000</td>\n      <td>2.000000</td>\n      <td>5.000000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>146.358780</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ckd</td>\n      <td>10.800000</td>\n      <td>1.900000</td>\n      <td>33.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>140.041412</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ckd</td>\n      <td>9.500000</td>\n      <td>7.200000</td>\n      <td>29.000000</td>\n      <td>2.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>114.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ckd</td>\n      <td>9.400000</td>\n      <td>4.000000</td>\n      <td>28.000000</td>\n      <td>2.000000</td>\n      <td>4.005585</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>128.284927</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ckd</td>\n      <td>10.800000</td>\n      <td>2.700000</td>\n      <td>32.000000</td>\n      <td>3.000000</td>\n      <td>3.800000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>131.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ckd</td>\n      <td>9.700000</td>\n      <td>2.100000</td>\n      <td>28.000000</td>\n      <td>3.000000</td>\n      <td>3.400000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ckd</td>\n      <td>9.800000</td>\n      <td>4.600000</td>\n      <td>33.895054</td>\n      <td>1.022104</td>\n      <td>4.085327</td>\n      <td>True</td>\n      <td>0.918048</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ckd</td>\n      <td>5.600000</td>\n      <td>4.100000</td>\n      <td>16.000000</td>\n      <td>3.000000</td>\n      <td>2.600000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>ckd</td>\n      <td>7.600000</td>\n      <td>9.600000</td>\n      <td>24.000000</td>\n      <td>3.000000</td>\n      <td>2.800000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>ckd</td>\n      <td>12.600000</td>\n      <td>2.200000</td>\n      <td>38.387016</td>\n      <td>2.000000</td>\n      <td>4.610007</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>ckd</td>\n      <td>12.100000</td>\n      <td>5.200000</td>\n      <td>37.542927</td>\n      <td>1.188392</td>\n      <td>4.511091</td>\n      <td>True</td>\n      <td>1.010967</td>\n      <td>no</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>ckd</td>\n      <td>12.700000</td>\n      <td>1.300000</td>\n      <td>37.000000</td>\n      <td>0.000000</td>\n      <td>4.300000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>ckd</td>\n      <td>10.300000</td>\n      <td>1.600000</td>\n      <td>30.000000</td>\n      <td>1.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>125.192413</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>ckd</td>\n      <td>7.700000</td>\n      <td>3.900000</td>\n      <td>24.000000</td>\n      <td>2.000000</td>\n      <td>3.200000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>ckd</td>\n      <td>10.900000</td>\n      <td>76.000000</td>\n      <td>32.000000</td>\n      <td>-0.386982</td>\n      <td>3.600000</td>\n      <td>True</td>\n      <td>0.458403</td>\n      <td>True</td>\n      <td>4.500000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>ckd</td>\n      <td>9.800000</td>\n      <td>7.700000</td>\n      <td>32.000000</td>\n      <td>4.000000</td>\n      <td>3.400000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>136.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>ckd</td>\n      <td>11.462144</td>\n      <td>3.217522</td>\n      <td>35.280643</td>\n      <td>0.000000</td>\n      <td>4.244498</td>\n      <td>no</td>\n      <td>1.010000</td>\n      <td>no</td>\n      <td>125.593338</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ckd</td>\n      <td>11.100000</td>\n      <td>1.400000</td>\n      <td>39.000000</td>\n      <td>4.000000</td>\n      <td>4.600000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>129.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>ckd</td>\n      <td>9.900000</td>\n      <td>1.900000</td>\n      <td>29.000000</td>\n      <td>0.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>True</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>ckd</td>\n      <td>11.600000</td>\n      <td>2.400000</td>\n      <td>35.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ckd</td>\n      <td>12.500000</td>\n      <td>2.700000</td>\n      <td>37.000000</td>\n      <td>3.000000</td>\n      <td>4.100000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>ckd</td>\n      <td>11.369183</td>\n      <td>1.400000</td>\n      <td>34.953079</td>\n      <td>1.000000</td>\n      <td>4.198009</td>\n      <td>no</td>\n      <td>0.920222</td>\n      <td>True</td>\n      <td>125.981575</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>ckd</td>\n      <td>12.900000</td>\n      <td>1.400000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>4.555233</td>\n      <td>no</td>\n      <td>1.010000</td>\n      <td>no</td>\n      <td>136.339951</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>370</th>\n      <td>notckd</td>\n      <td>16.200000</td>\n      <td>1.200000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>371</th>\n      <td>notckd</td>\n      <td>17.600000</td>\n      <td>0.500000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>372</th>\n      <td>notckd</td>\n      <td>15.000000</td>\n      <td>0.900000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>notckd</td>\n      <td>13.700000</td>\n      <td>1.000000</td>\n      <td>47.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>374</th>\n      <td>notckd</td>\n      <td>16.300000</td>\n      <td>1.200000</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>6.400000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>375</th>\n      <td>notckd</td>\n      <td>15.100000</td>\n      <td>0.500000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n      <td>5.600000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>143.000000</td>\n    </tr>\n    <tr>\n      <th>376</th>\n      <td>notckd</td>\n      <td>16.400000</td>\n      <td>1.100000</td>\n      <td>53.000000</td>\n      <td>0.000000</td>\n      <td>5.200000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>147.000000</td>\n    </tr>\n    <tr>\n      <th>377</th>\n      <td>notckd</td>\n      <td>13.800000</td>\n      <td>0.700000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>4.800000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>378</th>\n      <td>notckd</td>\n      <td>15.200000</td>\n      <td>0.900000</td>\n      <td>42.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>notckd</td>\n      <td>16.100000</td>\n      <td>0.600000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.700000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>notckd</td>\n      <td>15.300000</td>\n      <td>1.100000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>notckd</td>\n      <td>16.600000</td>\n      <td>0.500000</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>5.900000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>notckd</td>\n      <td>16.800000</td>\n      <td>0.800000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>6.500000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>137.000000</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>notckd</td>\n      <td>13.900000</td>\n      <td>0.700000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>notckd</td>\n      <td>15.400000</td>\n      <td>1.100000</td>\n      <td>42.000000</td>\n      <td>0.000000</td>\n      <td>4.500000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>385</th>\n      <td>notckd</td>\n      <td>16.500000</td>\n      <td>0.600000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.100000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>notckd</td>\n      <td>16.400000</td>\n      <td>0.500000</td>\n      <td>43.000000</td>\n      <td>0.000000</td>\n      <td>6.500000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>notckd</td>\n      <td>16.700000</td>\n      <td>0.900000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.200000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>136.000000</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>notckd</td>\n      <td>15.500000</td>\n      <td>1.200000</td>\n      <td>46.000000</td>\n      <td>0.000000</td>\n      <td>6.400000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>144.000000</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>notckd</td>\n      <td>17.000000</td>\n      <td>0.700000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.800000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>notckd</td>\n      <td>15.000000</td>\n      <td>0.800000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.300000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>notckd</td>\n      <td>15.600000</td>\n      <td>1.100000</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>6.300000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>notckd</td>\n      <td>14.800000</td>\n      <td>1.200000</td>\n      <td>46.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>147.000000</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>notckd</td>\n      <td>13.000000</td>\n      <td>0.700000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>notckd</td>\n      <td>14.100000</td>\n      <td>0.800000</td>\n      <td>45.000000</td>\n      <td>0.000000</td>\n      <td>4.600000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>notckd</td>\n      <td>15.700000</td>\n      <td>0.500000</td>\n      <td>47.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>notckd</td>\n      <td>16.500000</td>\n      <td>1.200000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>6.200000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>notckd</td>\n      <td>15.800000</td>\n      <td>0.600000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>no</td>\n      <td>1.020000</td>\n      <td>no</td>\n      <td>137.000000</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>notckd</td>\n      <td>14.200000</td>\n      <td>1.000000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>5.900000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>notckd</td>\n      <td>15.800000</td>\n      <td>1.100000</td>\n      <td>53.000000</td>\n      <td>0.000000</td>\n      <td>6.100000</td>\n      <td>no</td>\n      <td>1.030000</td>\n      <td>no</td>\n      <td>141.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 10 columns</p>\n</div>",
            "text/plain": "    classification       hemo         sc        pcv        al        rc   htn  \\\n0              ckd  15.400000   1.200000  44.000000  1.000000  5.200000  True   \n1              ckd  11.300000   0.800000  38.000000  4.000000  4.686921    no   \n2              ckd   9.600000   1.800000  31.000000  2.000000  4.114644    no   \n3              ckd  11.200000   3.800000  32.000000  4.000000  3.900000  True   \n4              ckd  11.600000   1.400000  35.000000  2.000000  4.600000    no   \n5              ckd  12.200000   1.100000  39.000000  3.000000  4.400000  True   \n6              ckd  12.400000  24.000000  36.000000  0.000000  4.188086    no   \n7              ckd  12.400000   1.100000  44.000000  2.000000  5.000000    no   \n8              ckd  10.800000   1.900000  33.000000  3.000000  4.000000  True   \n9              ckd   9.500000   7.200000  29.000000  2.000000  3.700000  True   \n10             ckd   9.400000   4.000000  28.000000  2.000000  4.005585  True   \n11             ckd  10.800000   2.700000  32.000000  3.000000  3.800000  True   \n12             ckd   9.700000   2.100000  28.000000  3.000000  3.400000  True   \n13             ckd   9.800000   4.600000  33.895054  1.022104  4.085327  True   \n14             ckd   5.600000   4.100000  16.000000  3.000000  2.600000  True   \n15             ckd   7.600000   9.600000  24.000000  3.000000  2.800000  True   \n16             ckd  12.600000   2.200000  38.387016  2.000000  4.610007    no   \n17             ckd  12.100000   5.200000  37.542927  1.188392  4.511091  True   \n18             ckd  12.700000   1.300000  37.000000  0.000000  4.300000  True   \n19             ckd  10.300000   1.600000  30.000000  1.000000  3.700000  True   \n20             ckd   7.700000   3.900000  24.000000  2.000000  3.200000  True   \n21             ckd  10.900000  76.000000  32.000000 -0.386982  3.600000  True   \n22             ckd   9.800000   7.700000  32.000000  4.000000  3.400000  True   \n23             ckd  11.462144   3.217522  35.280643  0.000000  4.244498    no   \n24             ckd  11.100000   1.400000  39.000000  4.000000  4.600000  True   \n25             ckd   9.900000   1.900000  29.000000  0.000000  3.700000  True   \n26             ckd  11.600000   2.400000  35.000000  0.000000  4.000000  True   \n27             ckd  12.500000   2.700000  37.000000  3.000000  4.100000  True   \n28             ckd  11.369183   1.400000  34.953079  1.000000  4.198009    no   \n29             ckd  12.900000   1.400000  38.000000  1.000000  4.555233    no   \n..             ...        ...        ...        ...       ...       ...   ...   \n370         notckd  16.200000   1.200000  50.000000  0.000000  5.400000    no   \n371         notckd  17.600000   0.500000  51.000000  0.000000  5.000000    no   \n372         notckd  15.000000   0.900000  52.000000  0.000000  5.500000    no   \n373         notckd  13.700000   1.000000  47.000000  0.000000  4.900000    no   \n374         notckd  16.300000   1.200000  40.000000  0.000000  6.400000    no   \n375         notckd  15.100000   0.500000  48.000000  0.000000  5.600000    no   \n376         notckd  16.400000   1.100000  53.000000  0.000000  5.200000    no   \n377         notckd  13.800000   0.700000  49.000000  0.000000  4.800000    no   \n378         notckd  15.200000   0.900000  42.000000  0.000000  5.500000    no   \n379         notckd  16.100000   0.600000  50.000000  0.000000  5.700000    no   \n380         notckd  15.300000   1.100000  54.000000  0.000000  4.900000    no   \n381         notckd  16.600000   0.500000  40.000000  0.000000  5.900000    no   \n382         notckd  16.800000   0.800000  51.000000  0.000000  6.500000    no   \n383         notckd  13.900000   0.700000  49.000000  0.000000  5.000000    no   \n384         notckd  15.400000   1.100000  42.000000  0.000000  4.500000    no   \n385         notckd  16.500000   0.600000  52.000000  0.000000  5.100000    no   \n386         notckd  16.400000   0.500000  43.000000  0.000000  6.500000    no   \n387         notckd  16.700000   0.900000  50.000000  0.000000  5.200000    no   \n388         notckd  15.500000   1.200000  46.000000  0.000000  6.400000    no   \n389         notckd  17.000000   0.700000  52.000000  0.000000  5.800000    no   \n390         notckd  15.000000   0.800000  52.000000  0.000000  5.300000    no   \n391         notckd  15.600000   1.100000  44.000000  0.000000  6.300000    no   \n392         notckd  14.800000   1.200000  46.000000  0.000000  5.500000    no   \n393         notckd  13.000000   0.700000  54.000000  0.000000  5.400000    no   \n394         notckd  14.100000   0.800000  45.000000  0.000000  4.600000    no   \n395         notckd  15.700000   0.500000  47.000000  0.000000  4.900000    no   \n396         notckd  16.500000   1.200000  54.000000  0.000000  6.200000    no   \n397         notckd  15.800000   0.600000  49.000000  0.000000  5.400000    no   \n398         notckd  14.200000   1.000000  51.000000  0.000000  5.900000    no   \n399         notckd  15.800000   1.100000  53.000000  0.000000  6.100000    no   \n\n           sg    dm         sod  \n0    1.020000  True  146.657867  \n1    1.020000    no  151.385254  \n2    1.010000  True  130.725586  \n3    1.010000    no  111.000000  \n4    1.010000    no  137.998795  \n5    1.020000  True  142.000000  \n6    1.010000    no  104.000000  \n7    1.020000  True  146.358780  \n8    1.020000  True  140.041412  \n9    1.020000  True  114.000000  \n10   1.010000  True  128.284927  \n11   1.010000  True  131.000000  \n12   1.020000  True  138.000000  \n13   0.918048  True  135.000000  \n14   1.010000  True  130.000000  \n15   1.020000    no  141.000000  \n16   1.020000    no  138.000000  \n17   1.010967    no  139.000000  \n18   1.030000  True  135.000000  \n19   1.020000    no  125.192413  \n20   1.020000  True  135.000000  \n21   0.458403  True    4.500000  \n22   1.030000    no  136.000000  \n23   1.010000    no  125.593338  \n24   1.020000    no  129.000000  \n25   1.030000  True  141.000000  \n26   1.020000  True  140.000000  \n27   1.010000  True  130.000000  \n28   0.920222  True  125.981575  \n29   1.010000    no  136.339951  \n..        ...   ...         ...  \n370  1.020000    no  139.000000  \n371  1.030000    no  145.000000  \n372  1.020000    no  150.000000  \n373  1.030000    no  142.000000  \n374  1.030000    no  146.000000  \n375  1.020000    no  143.000000  \n376  1.030000    no  147.000000  \n377  1.020000    no  145.000000  \n378  1.030000    no  140.000000  \n379  1.030000    no  138.000000  \n380  1.020000    no  139.000000  \n381  1.030000    no  142.000000  \n382  1.030000    no  137.000000  \n383  1.030000    no  141.000000  \n384  1.020000    no  150.000000  \n385  1.020000    no  146.000000  \n386  1.030000    no  142.000000  \n387  1.030000    no  136.000000  \n388  1.020000    no  144.000000  \n389  1.030000    no  140.000000  \n390  1.030000    no  135.000000  \n391  1.030000    no  142.000000  \n392  1.020000    no  147.000000  \n393  1.030000    no  141.000000  \n394  1.020000    no  139.000000  \n395  1.020000    no  150.000000  \n396  1.030000    no  141.000000  \n397  1.020000    no  137.000000  \n398  1.030000    no  135.000000  \n399  1.030000    no  141.000000  \n\n[400 rows x 10 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step - 2 Data Cleaning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First we will encode the category columns based on our own schema, and convert classes into numeric type which is recognisable. We will also remove any NaNs (missing values) from the data. We will be doing this all using the *Pandas library*."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "#cleaning the data - replacing yes with 1 and no with 0 in pandas dataframe\ndf['htn'] = df['htn'].replace('yes',True) #htn column\ndf['htn'] = df['htn'].replace('no',False)  #htn coloumn\ndf['dm'] = df['dm'].replace('yes',True)   #dm column\ndf['dm'] = df['dm'].replace('no',False)    #dm column\n\ndf['classification'] = df['classification'].replace('ckd',True)  #ckd = True\ndf['classification'] = df['classification'].replace('notckd',False)  #notckd = False\ndf = df.fillna(0) #removing any NaNs\n#seeing the cleaned data\ndf",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classification</th>\n      <th>hemo</th>\n      <th>sc</th>\n      <th>pcv</th>\n      <th>al</th>\n      <th>rc</th>\n      <th>htn</th>\n      <th>sg</th>\n      <th>dm</th>\n      <th>sod</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>True</td>\n      <td>15.400000</td>\n      <td>1.200000</td>\n      <td>44.000000</td>\n      <td>1.000000</td>\n      <td>5.200000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>146.657867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>11.300000</td>\n      <td>0.800000</td>\n      <td>38.000000</td>\n      <td>4.000000</td>\n      <td>4.686921</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>151.385254</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>True</td>\n      <td>9.600000</td>\n      <td>1.800000</td>\n      <td>31.000000</td>\n      <td>2.000000</td>\n      <td>4.114644</td>\n      <td>False</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.725586</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>True</td>\n      <td>11.200000</td>\n      <td>3.800000</td>\n      <td>32.000000</td>\n      <td>4.000000</td>\n      <td>3.900000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>False</td>\n      <td>111.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>True</td>\n      <td>11.600000</td>\n      <td>1.400000</td>\n      <td>35.000000</td>\n      <td>2.000000</td>\n      <td>4.600000</td>\n      <td>False</td>\n      <td>1.010000</td>\n      <td>False</td>\n      <td>137.998795</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>True</td>\n      <td>12.200000</td>\n      <td>1.100000</td>\n      <td>39.000000</td>\n      <td>3.000000</td>\n      <td>4.400000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>True</td>\n      <td>12.400000</td>\n      <td>24.000000</td>\n      <td>36.000000</td>\n      <td>0.000000</td>\n      <td>4.188086</td>\n      <td>False</td>\n      <td>1.010000</td>\n      <td>False</td>\n      <td>104.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>True</td>\n      <td>12.400000</td>\n      <td>1.100000</td>\n      <td>44.000000</td>\n      <td>2.000000</td>\n      <td>5.000000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>146.358780</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>True</td>\n      <td>10.800000</td>\n      <td>1.900000</td>\n      <td>33.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>140.041412</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>True</td>\n      <td>9.500000</td>\n      <td>7.200000</td>\n      <td>29.000000</td>\n      <td>2.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>114.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>True</td>\n      <td>9.400000</td>\n      <td>4.000000</td>\n      <td>28.000000</td>\n      <td>2.000000</td>\n      <td>4.005585</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>128.284927</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>True</td>\n      <td>10.800000</td>\n      <td>2.700000</td>\n      <td>32.000000</td>\n      <td>3.000000</td>\n      <td>3.800000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>131.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>True</td>\n      <td>9.700000</td>\n      <td>2.100000</td>\n      <td>28.000000</td>\n      <td>3.000000</td>\n      <td>3.400000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>True</td>\n      <td>9.800000</td>\n      <td>4.600000</td>\n      <td>33.895054</td>\n      <td>1.022104</td>\n      <td>4.085327</td>\n      <td>True</td>\n      <td>0.918048</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>True</td>\n      <td>5.600000</td>\n      <td>4.100000</td>\n      <td>16.000000</td>\n      <td>3.000000</td>\n      <td>2.600000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.000000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>True</td>\n      <td>7.600000</td>\n      <td>9.600000</td>\n      <td>24.000000</td>\n      <td>3.000000</td>\n      <td>2.800000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>True</td>\n      <td>12.600000</td>\n      <td>2.200000</td>\n      <td>38.387016</td>\n      <td>2.000000</td>\n      <td>4.610007</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>True</td>\n      <td>12.100000</td>\n      <td>5.200000</td>\n      <td>37.542927</td>\n      <td>1.188392</td>\n      <td>4.511091</td>\n      <td>True</td>\n      <td>1.010967</td>\n      <td>False</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>True</td>\n      <td>12.700000</td>\n      <td>1.300000</td>\n      <td>37.000000</td>\n      <td>0.000000</td>\n      <td>4.300000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>True</td>\n      <td>10.300000</td>\n      <td>1.600000</td>\n      <td>30.000000</td>\n      <td>1.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>125.192413</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>True</td>\n      <td>7.700000</td>\n      <td>3.900000</td>\n      <td>24.000000</td>\n      <td>2.000000</td>\n      <td>3.200000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>True</td>\n      <td>10.900000</td>\n      <td>76.000000</td>\n      <td>32.000000</td>\n      <td>-0.386982</td>\n      <td>3.600000</td>\n      <td>True</td>\n      <td>0.458403</td>\n      <td>True</td>\n      <td>4.500000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>True</td>\n      <td>9.800000</td>\n      <td>7.700000</td>\n      <td>32.000000</td>\n      <td>4.000000</td>\n      <td>3.400000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>136.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>True</td>\n      <td>11.462144</td>\n      <td>3.217522</td>\n      <td>35.280643</td>\n      <td>0.000000</td>\n      <td>4.244498</td>\n      <td>False</td>\n      <td>1.010000</td>\n      <td>False</td>\n      <td>125.593338</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>True</td>\n      <td>11.100000</td>\n      <td>1.400000</td>\n      <td>39.000000</td>\n      <td>4.000000</td>\n      <td>4.600000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>129.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>True</td>\n      <td>9.900000</td>\n      <td>1.900000</td>\n      <td>29.000000</td>\n      <td>0.000000</td>\n      <td>3.700000</td>\n      <td>True</td>\n      <td>1.030000</td>\n      <td>True</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>True</td>\n      <td>11.600000</td>\n      <td>2.400000</td>\n      <td>35.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>True</td>\n      <td>1.020000</td>\n      <td>True</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>True</td>\n      <td>12.500000</td>\n      <td>2.700000</td>\n      <td>37.000000</td>\n      <td>3.000000</td>\n      <td>4.100000</td>\n      <td>True</td>\n      <td>1.010000</td>\n      <td>True</td>\n      <td>130.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>True</td>\n      <td>11.369183</td>\n      <td>1.400000</td>\n      <td>34.953079</td>\n      <td>1.000000</td>\n      <td>4.198009</td>\n      <td>False</td>\n      <td>0.920222</td>\n      <td>True</td>\n      <td>125.981575</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>True</td>\n      <td>12.900000</td>\n      <td>1.400000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>4.555233</td>\n      <td>False</td>\n      <td>1.010000</td>\n      <td>False</td>\n      <td>136.339951</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>370</th>\n      <td>False</td>\n      <td>16.200000</td>\n      <td>1.200000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>371</th>\n      <td>False</td>\n      <td>17.600000</td>\n      <td>0.500000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>372</th>\n      <td>False</td>\n      <td>15.000000</td>\n      <td>0.900000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>False</td>\n      <td>13.700000</td>\n      <td>1.000000</td>\n      <td>47.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>374</th>\n      <td>False</td>\n      <td>16.300000</td>\n      <td>1.200000</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>6.400000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>375</th>\n      <td>False</td>\n      <td>15.100000</td>\n      <td>0.500000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n      <td>5.600000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>143.000000</td>\n    </tr>\n    <tr>\n      <th>376</th>\n      <td>False</td>\n      <td>16.400000</td>\n      <td>1.100000</td>\n      <td>53.000000</td>\n      <td>0.000000</td>\n      <td>5.200000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>147.000000</td>\n    </tr>\n    <tr>\n      <th>377</th>\n      <td>False</td>\n      <td>13.800000</td>\n      <td>0.700000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>4.800000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>378</th>\n      <td>False</td>\n      <td>15.200000</td>\n      <td>0.900000</td>\n      <td>42.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>False</td>\n      <td>16.100000</td>\n      <td>0.600000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.700000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>138.000000</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>False</td>\n      <td>15.300000</td>\n      <td>1.100000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>False</td>\n      <td>16.600000</td>\n      <td>0.500000</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>5.900000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>False</td>\n      <td>16.800000</td>\n      <td>0.800000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>6.500000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>137.000000</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>False</td>\n      <td>13.900000</td>\n      <td>0.700000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>False</td>\n      <td>15.400000</td>\n      <td>1.100000</td>\n      <td>42.000000</td>\n      <td>0.000000</td>\n      <td>4.500000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>385</th>\n      <td>False</td>\n      <td>16.500000</td>\n      <td>0.600000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.100000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>False</td>\n      <td>16.400000</td>\n      <td>0.500000</td>\n      <td>43.000000</td>\n      <td>0.000000</td>\n      <td>6.500000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>False</td>\n      <td>16.700000</td>\n      <td>0.900000</td>\n      <td>50.000000</td>\n      <td>0.000000</td>\n      <td>5.200000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>136.000000</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>False</td>\n      <td>15.500000</td>\n      <td>1.200000</td>\n      <td>46.000000</td>\n      <td>0.000000</td>\n      <td>6.400000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>144.000000</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>False</td>\n      <td>17.000000</td>\n      <td>0.700000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.800000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>140.000000</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>False</td>\n      <td>15.000000</td>\n      <td>0.800000</td>\n      <td>52.000000</td>\n      <td>0.000000</td>\n      <td>5.300000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>False</td>\n      <td>15.600000</td>\n      <td>1.100000</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>6.300000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>142.000000</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>False</td>\n      <td>14.800000</td>\n      <td>1.200000</td>\n      <td>46.000000</td>\n      <td>0.000000</td>\n      <td>5.500000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>147.000000</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>False</td>\n      <td>13.000000</td>\n      <td>0.700000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>False</td>\n      <td>14.100000</td>\n      <td>0.800000</td>\n      <td>45.000000</td>\n      <td>0.000000</td>\n      <td>4.600000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>139.000000</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>False</td>\n      <td>15.700000</td>\n      <td>0.500000</td>\n      <td>47.000000</td>\n      <td>0.000000</td>\n      <td>4.900000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>False</td>\n      <td>16.500000</td>\n      <td>1.200000</td>\n      <td>54.000000</td>\n      <td>0.000000</td>\n      <td>6.200000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>141.000000</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>False</td>\n      <td>15.800000</td>\n      <td>0.600000</td>\n      <td>49.000000</td>\n      <td>0.000000</td>\n      <td>5.400000</td>\n      <td>False</td>\n      <td>1.020000</td>\n      <td>False</td>\n      <td>137.000000</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>False</td>\n      <td>14.200000</td>\n      <td>1.000000</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>5.900000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>False</td>\n      <td>15.800000</td>\n      <td>1.100000</td>\n      <td>53.000000</td>\n      <td>0.000000</td>\n      <td>6.100000</td>\n      <td>False</td>\n      <td>1.030000</td>\n      <td>False</td>\n      <td>141.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 10 columns</p>\n</div>",
            "text/plain": "     classification       hemo         sc        pcv        al        rc  \\\n0              True  15.400000   1.200000  44.000000  1.000000  5.200000   \n1              True  11.300000   0.800000  38.000000  4.000000  4.686921   \n2              True   9.600000   1.800000  31.000000  2.000000  4.114644   \n3              True  11.200000   3.800000  32.000000  4.000000  3.900000   \n4              True  11.600000   1.400000  35.000000  2.000000  4.600000   \n5              True  12.200000   1.100000  39.000000  3.000000  4.400000   \n6              True  12.400000  24.000000  36.000000  0.000000  4.188086   \n7              True  12.400000   1.100000  44.000000  2.000000  5.000000   \n8              True  10.800000   1.900000  33.000000  3.000000  4.000000   \n9              True   9.500000   7.200000  29.000000  2.000000  3.700000   \n10             True   9.400000   4.000000  28.000000  2.000000  4.005585   \n11             True  10.800000   2.700000  32.000000  3.000000  3.800000   \n12             True   9.700000   2.100000  28.000000  3.000000  3.400000   \n13             True   9.800000   4.600000  33.895054  1.022104  4.085327   \n14             True   5.600000   4.100000  16.000000  3.000000  2.600000   \n15             True   7.600000   9.600000  24.000000  3.000000  2.800000   \n16             True  12.600000   2.200000  38.387016  2.000000  4.610007   \n17             True  12.100000   5.200000  37.542927  1.188392  4.511091   \n18             True  12.700000   1.300000  37.000000  0.000000  4.300000   \n19             True  10.300000   1.600000  30.000000  1.000000  3.700000   \n20             True   7.700000   3.900000  24.000000  2.000000  3.200000   \n21             True  10.900000  76.000000  32.000000 -0.386982  3.600000   \n22             True   9.800000   7.700000  32.000000  4.000000  3.400000   \n23             True  11.462144   3.217522  35.280643  0.000000  4.244498   \n24             True  11.100000   1.400000  39.000000  4.000000  4.600000   \n25             True   9.900000   1.900000  29.000000  0.000000  3.700000   \n26             True  11.600000   2.400000  35.000000  0.000000  4.000000   \n27             True  12.500000   2.700000  37.000000  3.000000  4.100000   \n28             True  11.369183   1.400000  34.953079  1.000000  4.198009   \n29             True  12.900000   1.400000  38.000000  1.000000  4.555233   \n..              ...        ...        ...        ...       ...       ...   \n370           False  16.200000   1.200000  50.000000  0.000000  5.400000   \n371           False  17.600000   0.500000  51.000000  0.000000  5.000000   \n372           False  15.000000   0.900000  52.000000  0.000000  5.500000   \n373           False  13.700000   1.000000  47.000000  0.000000  4.900000   \n374           False  16.300000   1.200000  40.000000  0.000000  6.400000   \n375           False  15.100000   0.500000  48.000000  0.000000  5.600000   \n376           False  16.400000   1.100000  53.000000  0.000000  5.200000   \n377           False  13.800000   0.700000  49.000000  0.000000  4.800000   \n378           False  15.200000   0.900000  42.000000  0.000000  5.500000   \n379           False  16.100000   0.600000  50.000000  0.000000  5.700000   \n380           False  15.300000   1.100000  54.000000  0.000000  4.900000   \n381           False  16.600000   0.500000  40.000000  0.000000  5.900000   \n382           False  16.800000   0.800000  51.000000  0.000000  6.500000   \n383           False  13.900000   0.700000  49.000000  0.000000  5.000000   \n384           False  15.400000   1.100000  42.000000  0.000000  4.500000   \n385           False  16.500000   0.600000  52.000000  0.000000  5.100000   \n386           False  16.400000   0.500000  43.000000  0.000000  6.500000   \n387           False  16.700000   0.900000  50.000000  0.000000  5.200000   \n388           False  15.500000   1.200000  46.000000  0.000000  6.400000   \n389           False  17.000000   0.700000  52.000000  0.000000  5.800000   \n390           False  15.000000   0.800000  52.000000  0.000000  5.300000   \n391           False  15.600000   1.100000  44.000000  0.000000  6.300000   \n392           False  14.800000   1.200000  46.000000  0.000000  5.500000   \n393           False  13.000000   0.700000  54.000000  0.000000  5.400000   \n394           False  14.100000   0.800000  45.000000  0.000000  4.600000   \n395           False  15.700000   0.500000  47.000000  0.000000  4.900000   \n396           False  16.500000   1.200000  54.000000  0.000000  6.200000   \n397           False  15.800000   0.600000  49.000000  0.000000  5.400000   \n398           False  14.200000   1.000000  51.000000  0.000000  5.900000   \n399           False  15.800000   1.100000  53.000000  0.000000  6.100000   \n\n       htn        sg     dm         sod  \n0     True  1.020000   True  146.657867  \n1    False  1.020000  False  151.385254  \n2    False  1.010000   True  130.725586  \n3     True  1.010000  False  111.000000  \n4    False  1.010000  False  137.998795  \n5     True  1.020000   True  142.000000  \n6    False  1.010000  False  104.000000  \n7    False  1.020000   True  146.358780  \n8     True  1.020000   True  140.041412  \n9     True  1.020000   True  114.000000  \n10    True  1.010000   True  128.284927  \n11    True  1.010000   True  131.000000  \n12    True  1.020000   True  138.000000  \n13    True  0.918048   True  135.000000  \n14    True  1.010000   True  130.000000  \n15    True  1.020000  False  141.000000  \n16   False  1.020000  False  138.000000  \n17    True  1.010967  False  139.000000  \n18    True  1.030000   True  135.000000  \n19    True  1.020000  False  125.192413  \n20    True  1.020000   True  135.000000  \n21    True  0.458403   True    4.500000  \n22    True  1.030000  False  136.000000  \n23   False  1.010000  False  125.593338  \n24    True  1.020000  False  129.000000  \n25    True  1.030000   True  141.000000  \n26    True  1.020000   True  140.000000  \n27    True  1.010000   True  130.000000  \n28   False  0.920222   True  125.981575  \n29   False  1.010000  False  136.339951  \n..     ...       ...    ...         ...  \n370  False  1.020000  False  139.000000  \n371  False  1.030000  False  145.000000  \n372  False  1.020000  False  150.000000  \n373  False  1.030000  False  142.000000  \n374  False  1.030000  False  146.000000  \n375  False  1.020000  False  143.000000  \n376  False  1.030000  False  147.000000  \n377  False  1.020000  False  145.000000  \n378  False  1.030000  False  140.000000  \n379  False  1.030000  False  138.000000  \n380  False  1.020000  False  139.000000  \n381  False  1.030000  False  142.000000  \n382  False  1.030000  False  137.000000  \n383  False  1.030000  False  141.000000  \n384  False  1.020000  False  150.000000  \n385  False  1.020000  False  146.000000  \n386  False  1.030000  False  142.000000  \n387  False  1.030000  False  136.000000  \n388  False  1.020000  False  144.000000  \n389  False  1.030000  False  140.000000  \n390  False  1.030000  False  135.000000  \n391  False  1.030000  False  142.000000  \n392  False  1.020000  False  147.000000  \n393  False  1.030000  False  141.000000  \n394  False  1.020000  False  139.000000  \n395  False  1.020000  False  150.000000  \n396  False  1.030000  False  141.000000  \n397  False  1.020000  False  137.000000  \n398  False  1.030000  False  135.000000  \n399  False  1.030000  False  141.000000  \n\n[400 rows x 10 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us import some basic dependencies for creatinng and training our model using PyTorch and evaluate and serialize deta using sklearn."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import from_numpy as totensor\n\nfrom sklearn.preprocessing import StandardScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The **classification** column is the target for our model, and so shall we change into a category and encode it to numeric classes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['classification'] = df['classification'].astype('category')\nencode_map = {\n    'ckd': 1,\n    'notckd': 0\n}\n\ndf['classification'].replace(encode_map, inplace=True)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = df.iloc[:, 1:10]\ny = df.iloc[:, 0:1]\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=70)\n#splitting the test data and train data randomly from the input. Train data = 70% of all samples.",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n  return self.fit(X, **fit_params).transform(X)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype float64, object were all converted to float64 by StandardScaler.\n  return self.fit(X, **fit_params).transform(X)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Now let us define the parameters of our training\nEPOCHS = 150\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0001",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## First we wll convert them into numpy array and then into PyTorch Tensors\nxx,yy=np.array(X_train, dtype='float32'),np.array(Y_train, dtype='float32')",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## train data\nclass CustomDataset(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\nxx,yy = torch.FloatTensor(xx), torch.FloatTensor(yy)\ntrain_data = CustomDataset(xx,yy)\n## test data    \nclass testData(Dataset):\n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n    def __len__ (self):\n        return len(self.X_data)\n\ntest_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(np.array(Y_test, dtype='float32')))",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let us now use the data loader which we created in the previous block."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here, we are defining the architecture of our model. We are going to have a 9-(32-32)-1 Neural network with 2 batch normalization, a drouput layer and relu as the activation function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Perceptron(torch.nn.Module):\n    def __init__(self):\n        super(Perceptron, self).__init__()\n        self.layer_1 = nn.Linear(9, 32) \n        self.layer_2 = nn.Linear(32, 32)\n        self.layer_out = nn.Linear(32, 1) \n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(32)\n        self.batchnorm2 = nn.BatchNorm1d(32)\n    def forward(self, x):\n        x = self.relu(self.layer_1(x))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        return x",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model = Perceptron()\nprint(model)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Perceptron(\n  (layer_1): Linear(in_features=9, out_features=32, bias=True)\n  (layer_2): Linear(in_features=32, out_features=32, bias=True)\n  (layer_out): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n  (batchnorm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Now let us define the accuracy calculation function\ndef binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 3 Model Training\nNow we will turn into the train mode, and train the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.train()\nfor e in range(1, EPOCHS+1):\n    epoch_loss = 0\n    epoch_acc = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        \n        y_pred = model(X_batch)\n        \n        loss = criterion(y_pred, y_batch)\n        acc = binary_acc(y_pred, y_batch)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 001: | Loss: 0.00096 | Acc: 100.000\nEpoch 002: | Loss: 0.00071 | Acc: 100.000\nEpoch 003: | Loss: 0.00043 | Acc: 100.000\nEpoch 004: | Loss: 0.00089 | Acc: 100.000\nEpoch 005: | Loss: 0.00073 | Acc: 100.000\nEpoch 006: | Loss: 0.00077 | Acc: 100.000\nEpoch 007: | Loss: 0.00333 | Acc: 100.000\nEpoch 008: | Loss: 0.00453 | Acc: 99.600\nEpoch 009: | Loss: 0.00018 | Acc: 100.000\nEpoch 010: | Loss: 0.00073 | Acc: 100.000\nEpoch 011: | Loss: 0.00416 | Acc: 100.000\nEpoch 012: | Loss: 0.00021 | Acc: 100.000\nEpoch 013: | Loss: 0.00149 | Acc: 100.000\nEpoch 014: | Loss: 0.00119 | Acc: 100.000\nEpoch 015: | Loss: 0.00091 | Acc: 100.000\nEpoch 016: | Loss: 0.00032 | Acc: 100.000\nEpoch 017: | Loss: 0.00088 | Acc: 100.000\nEpoch 018: | Loss: 0.00310 | Acc: 100.000\nEpoch 019: | Loss: 0.00056 | Acc: 100.000\nEpoch 020: | Loss: 0.00066 | Acc: 100.000\nEpoch 021: | Loss: 0.00776 | Acc: 99.600\nEpoch 022: | Loss: 0.00065 | Acc: 100.000\nEpoch 023: | Loss: 0.00313 | Acc: 100.000\nEpoch 024: | Loss: 0.00090 | Acc: 100.000\nEpoch 025: | Loss: 0.00034 | Acc: 100.000\nEpoch 026: | Loss: 0.00260 | Acc: 100.000\nEpoch 027: | Loss: 0.00174 | Acc: 100.000\nEpoch 028: | Loss: 0.01354 | Acc: 99.200\nEpoch 029: | Loss: 0.00493 | Acc: 100.000\nEpoch 030: | Loss: 0.00150 | Acc: 100.000\nEpoch 031: | Loss: 0.00722 | Acc: 99.200\nEpoch 032: | Loss: 0.00684 | Acc: 99.600\nEpoch 033: | Loss: 0.00847 | Acc: 99.600\nEpoch 034: | Loss: 0.00067 | Acc: 100.000\nEpoch 035: | Loss: 0.00548 | Acc: 99.600\nEpoch 036: | Loss: 0.00097 | Acc: 100.000\nEpoch 037: | Loss: 0.00573 | Acc: 100.000\nEpoch 038: | Loss: 0.00088 | Acc: 100.000\nEpoch 039: | Loss: 0.00073 | Acc: 100.000\nEpoch 040: | Loss: 0.00437 | Acc: 99.600\nEpoch 041: | Loss: 0.00293 | Acc: 100.000\nEpoch 042: | Loss: 0.02223 | Acc: 99.200\nEpoch 043: | Loss: 0.00047 | Acc: 100.000\nEpoch 044: | Loss: 0.00894 | Acc: 99.200\nEpoch 045: | Loss: 0.00056 | Acc: 100.000\nEpoch 046: | Loss: 0.00138 | Acc: 100.000\nEpoch 047: | Loss: 0.02611 | Acc: 99.600\nEpoch 048: | Loss: 0.00049 | Acc: 100.000\nEpoch 049: | Loss: 0.01247 | Acc: 99.200\nEpoch 050: | Loss: 0.00624 | Acc: 99.600\nEpoch 051: | Loss: 0.00108 | Acc: 100.000\nEpoch 052: | Loss: 0.00439 | Acc: 99.600\nEpoch 053: | Loss: 0.00302 | Acc: 100.000\nEpoch 054: | Loss: 0.00088 | Acc: 100.000\nEpoch 055: | Loss: 0.00120 | Acc: 100.000\nEpoch 056: | Loss: 0.00491 | Acc: 100.000\nEpoch 057: | Loss: 0.00316 | Acc: 100.000\nEpoch 058: | Loss: 0.00053 | Acc: 100.000\nEpoch 059: | Loss: 0.00084 | Acc: 100.000\nEpoch 060: | Loss: 0.00138 | Acc: 100.000\nEpoch 061: | Loss: 0.00101 | Acc: 100.000\nEpoch 062: | Loss: 0.00100 | Acc: 100.000\nEpoch 063: | Loss: 0.00273 | Acc: 100.000\nEpoch 064: | Loss: 0.00040 | Acc: 100.000\nEpoch 065: | Loss: 0.00122 | Acc: 100.000\nEpoch 066: | Loss: 0.00345 | Acc: 100.000\nEpoch 067: | Loss: 0.00273 | Acc: 100.000\nEpoch 068: | Loss: 0.00116 | Acc: 100.000\nEpoch 069: | Loss: 0.00274 | Acc: 100.000\nEpoch 070: | Loss: 0.00443 | Acc: 99.600\nEpoch 071: | Loss: 0.00025 | Acc: 100.000\nEpoch 072: | Loss: 0.00843 | Acc: 99.600\nEpoch 073: | Loss: 0.00570 | Acc: 100.000\nEpoch 074: | Loss: 0.00094 | Acc: 100.000\nEpoch 075: | Loss: 0.00219 | Acc: 100.000\nEpoch 076: | Loss: 0.00311 | Acc: 100.000\nEpoch 077: | Loss: 0.00202 | Acc: 100.000\nEpoch 078: | Loss: 0.00111 | Acc: 100.000\nEpoch 079: | Loss: 0.00267 | Acc: 100.000\nEpoch 080: | Loss: 0.00055 | Acc: 100.000\nEpoch 081: | Loss: 0.00042 | Acc: 100.000\nEpoch 082: | Loss: 0.00049 | Acc: 100.000\nEpoch 083: | Loss: 0.00941 | Acc: 99.200\nEpoch 084: | Loss: 0.00135 | Acc: 100.000\nEpoch 085: | Loss: 0.00053 | Acc: 100.000\nEpoch 086: | Loss: 0.00174 | Acc: 100.000\nEpoch 087: | Loss: 0.00501 | Acc: 99.600\nEpoch 088: | Loss: 0.00277 | Acc: 100.000\nEpoch 089: | Loss: 0.00890 | Acc: 99.600\nEpoch 090: | Loss: 0.00071 | Acc: 100.000\nEpoch 091: | Loss: 0.00128 | Acc: 100.000\nEpoch 092: | Loss: 0.01658 | Acc: 99.200\nEpoch 093: | Loss: 0.00129 | Acc: 100.000\nEpoch 094: | Loss: 0.00143 | Acc: 100.000\nEpoch 095: | Loss: 0.00202 | Acc: 100.000\nEpoch 096: | Loss: 0.00185 | Acc: 100.000\nEpoch 097: | Loss: 0.00042 | Acc: 100.000\nEpoch 098: | Loss: 0.00261 | Acc: 100.000\nEpoch 099: | Loss: 0.00148 | Acc: 100.000\nEpoch 100: | Loss: 0.00053 | Acc: 100.000\nEpoch 101: | Loss: 0.00033 | Acc: 100.000\nEpoch 102: | Loss: 0.00085 | Acc: 100.000\nEpoch 103: | Loss: 0.00199 | Acc: 100.000\nEpoch 104: | Loss: 0.00094 | Acc: 100.000\nEpoch 105: | Loss: 0.00341 | Acc: 100.000\nEpoch 106: | Loss: 0.01028 | Acc: 99.200\nEpoch 107: | Loss: 0.01477 | Acc: 99.200\nEpoch 108: | Loss: 0.00298 | Acc: 100.000\nEpoch 109: | Loss: 0.00542 | Acc: 99.600\nEpoch 110: | Loss: 0.02212 | Acc: 99.200\nEpoch 111: | Loss: 0.00648 | Acc: 100.000\nEpoch 112: | Loss: 0.00779 | Acc: 99.600\nEpoch 113: | Loss: 0.00409 | Acc: 100.000\nEpoch 114: | Loss: 0.00168 | Acc: 100.000\nEpoch 115: | Loss: 0.00150 | Acc: 100.000\nEpoch 116: | Loss: 0.00606 | Acc: 99.600\nEpoch 117: | Loss: 0.00498 | Acc: 99.600\nEpoch 118: | Loss: 0.00475 | Acc: 99.600\nEpoch 119: | Loss: 0.00435 | Acc: 100.000\nEpoch 120: | Loss: 0.00379 | Acc: 100.000\nEpoch 121: | Loss: 0.00071 | Acc: 100.000\nEpoch 122: | Loss: 0.01289 | Acc: 99.200\nEpoch 123: | Loss: 0.00073 | Acc: 100.000\nEpoch 124: | Loss: 0.00039 | Acc: 100.000\nEpoch 125: | Loss: 0.00323 | Acc: 100.000\nEpoch 126: | Loss: 0.00061 | Acc: 100.000\nEpoch 127: | Loss: 0.00058 | Acc: 100.000\nEpoch 128: | Loss: 0.00082 | Acc: 100.000\nEpoch 129: | Loss: 0.00091 | Acc: 100.000\nEpoch 130: | Loss: 0.00030 | Acc: 100.000\nEpoch 131: | Loss: 0.00058 | Acc: 100.000\nEpoch 132: | Loss: 0.00124 | Acc: 100.000\nEpoch 133: | Loss: 0.00017 | Acc: 100.000\nEpoch 134: | Loss: 0.00199 | Acc: 100.000\nEpoch 135: | Loss: 0.00406 | Acc: 99.600\nEpoch 136: | Loss: 0.00082 | Acc: 100.000\nEpoch 137: | Loss: 0.00211 | Acc: 100.000\nEpoch 138: | Loss: 0.00051 | Acc: 100.000\nEpoch 139: | Loss: 0.00875 | Acc: 99.200\nEpoch 140: | Loss: 0.00102 | Acc: 100.000\nEpoch 141: | Loss: 0.00177 | Acc: 100.000\nEpoch 142: | Loss: 0.00040 | Acc: 100.000\nEpoch 143: | Loss: 0.00124 | Acc: 100.000\nEpoch 144: | Loss: 0.00164 | Acc: 100.000\nEpoch 145: | Loss: 0.00284 | Acc: 100.000\nEpoch 146: | Loss: 0.00109 | Acc: 100.000\nEpoch 147: | Loss: 0.00066 | Acc: 100.000\nEpoch 148: | Loss: 0.00040 | Acc: 100.000\nEpoch 149: | Loss: 0.00574 | Acc: 99.600\nEpoch 150: | Loss: 0.00076 | Acc: 100.000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see that the loss has reduced repeatedly and the train accuracy booming to 99.6% Now we will test the model over a test data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.eval()\ntp,tn,fp,fn=0,0,0,0\n\nfor x,y in test_loader:\n    if torch.round(torch.sigmoid(model(x)))==y and y[0][0]==1.:\n        tp +=1\n    elif torch.round(torch.sigmoid(model(x)))==y and y[0][0]==0.:\n        tn+=1\n    elif torch.round(torch.sigmoid(model(x)))!=y and y[0][0]==1.:\n        fn+=1\n    elif torch.round(torch.sigmoid(model(x)))!=y and y[0][0]==0.:\n        fp+=1\nprint([[tp,fp],[fn,tn]])\nprint('\\n---------------------------------\\n')\nprint('Validation accuracy = \\t'+str((tp+tn)/(tp+tn+fp+fn)*100)+'%')",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[73, 0], [2, 45]]\n\n---------------------------------\n\nValidation accuracy = \t98.33333333333333%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Above we can see from the test data (which is unseen to the model before),\nTrue Positive    =  73,\nFalse Positives  =   0,\nFalse Negatives  =   2,\nTrue Negatives   =  45.\n\nAnd the valiation accuracy = 98.33%"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Saving and loading our model\n\nNow, we have seen that the accuracy is awesome over both training and test data we will save this model so that we can use it anytime in future. After saving we can easily make a checkpoint at this stage, and run the following cells without initializing again by just loading our saved model!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "torch.save(model,'nephron-ai-model.pth')",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Perceptron. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let us load our existing saved model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "loadmodel = torch.load('nephron-ai-model.pth')\nloadmodel.eval()",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 70,
          "data": {
            "text/plain": "Perceptron(\n  (layer_1): Linear(in_features=9, out_features=32, bias=True)\n  (layer_2): Linear(in_features=32, out_features=32, bias=True)\n  (layer_out): Linear(in_features=32, out_features=1, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n  (batchnorm1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}